{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Pytorch Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ref: https://www.kaggle.com/sironghuang/understanding-pytorch-hooks/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Toy example to understand Pytorch hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](fig.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (s1): Sigmoid()\n",
      "  (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (s2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # y = xA^T + b \n",
    "        self.s1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.s2 = nn.Sigmoid()\n",
    "        self.fc1.weight = torch.nn.Parameter(torch.Tensor([[0.15, 0.25],\n",
    "                                                           [0.20, 0.30]]))\n",
    "        self.fc1.bias = torch.nn.Parameter(torch.Tensor([0.35]))\n",
    "        self.fc2.weight = torch.nn.Parameter(torch.Tensor([[0.40, 0.50],\n",
    "                                                           [0.45, 0.55]]))\n",
    "        self.fc2.bias = torch.nn.Parameter(torch.Tensor([0.6]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.fc1(x)\n",
    "        x = self.s1(x)\n",
    "        x= self.fc2(x)\n",
    "        x = self.s2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1500, 0.2500],\n",
      "        [0.2000, 0.3000]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3500], requires_grad=True), Parameter containing:\n",
      "tensor([[0.4000, 0.5000],\n",
      "        [0.4500, 0.5500]], requires_grad=True), Parameter containing:\n",
      "tensor([0.6000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# parameters: weight and bias\n",
    "weight1 = list(net.parameters())[0]\n",
    "weight2 = list(net.parameters())[2]\n",
    "\n",
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  : tensor([[0.0500, 0.1000]])\n",
      "target: tensor([[0.0100, 0.9900]])\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "data = torch.Tensor([0.05, 0.1])\n",
    "data = torch.unsqueeze(data, dim=0)\n",
    "data.required_grad = True\n",
    "\n",
    "target = torch.Tensor([0.01, 0.99])  # a dummy target, for example\n",
    "target = torch.unsqueeze(target, dim=0)\n",
    "\n",
    "print('data  :', data)\n",
    "print('target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out : tensor([[0.7569, 0.7677]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = net(data)\n",
    "print('out :', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        self.module = module\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fw)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_bw)\n",
    "            \n",
    "    def hook_fw(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        \n",
    "    def hook_bw(self, module, grad_in, grad_out):\n",
    "        self.input = grad_in\n",
    "        self.output = grad_out\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fc1', Linear(in_features=2, out_features=2, bias=True)),\n",
       " ('s1', Sigmoid()),\n",
       " ('fc2', Linear(in_features=2, out_features=2, bias=True)),\n",
       " ('s2', Sigmoid())]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net._modules.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********  Forward Hooks Inputs & Outputs  *********\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "layer 1, input : (tensor([[0.0500, 0.1000]]),)\n",
      "layer 1, output: tensor([[0.3825, 0.3900]], grad_fn=<ThAddmmBackward>)\n",
      "---------------------------------------------------\n",
      "Sigmoid()\n",
      "layer 2, input : (tensor([[0.3825, 0.3900]], grad_fn=<ThAddmmBackward>),)\n",
      "layer 2, output: tensor([[0.5945, 0.5963]], grad_fn=<SigmoidBackward>)\n",
      "---------------------------------------------------\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "layer 3, input : (tensor([[0.5945, 0.5963]], grad_fn=<SigmoidBackward>),)\n",
      "layer 3, output: tensor([[1.1359, 1.1955]], grad_fn=<ThAddmmBackward>)\n",
      "---------------------------------------------------\n",
      "Sigmoid()\n",
      "layer 4, input : (tensor([[1.1359, 1.1955]], grad_fn=<ThAddmmBackward>),)\n",
      "layer 4, output: tensor([[0.7569, 0.7677]], grad_fn=<SigmoidBackward>)\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "*********  Backward Hooks Inputs & Outputs  *********\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "layer 1, input : (tensor([[0.0371, 0.0458]]), None, tensor([[0.0019, 0.0023],\n",
      "        [0.0037, 0.0046]]))\n",
      "layer 1, output: (tensor([[0.0371, 0.0458]]),)\n",
      "---------------------------------------------------\n",
      "Sigmoid()\n",
      "layer 2, input : (tensor([[0.0371, 0.0458]]),)\n",
      "layer 2, output: (tensor([[0.1538, 0.1901]]),)\n",
      "---------------------------------------------------\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "layer 3, input : (tensor([[0.1840, 0.1783]]), tensor([[0.1538, 0.1901]]), tensor([[0.1094, 0.1060],\n",
      "        [0.1097, 0.1063]]))\n",
      "layer 3, output: (tensor([[0.1840, 0.1783]]),)\n",
      "---------------------------------------------------\n",
      "Sigmoid()\n",
      "layer 4, input : (tensor([[0.1840, 0.1783]]),)\n",
      "layer 4, output: (tensor([[1., 1.]]),)\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "*********      Gradients of parameters      *********\n",
      "fc1.weight\n",
      "tensor([[0.0019, 0.0037],\n",
      "        [0.0023, 0.0046]])\n",
      "fc1.bias\n",
      "tensor([0.0828])\n",
      "fc2.weight\n",
      "tensor([[0.1094, 0.1097],\n",
      "        [0.1060, 0.1063]])\n",
      "fc2.bias\n",
      "tensor([0.3623])\n"
     ]
    }
   ],
   "source": [
    "# register hooks on each layer\n",
    "# layer[0] is the name, and layer[1] is the instance\n",
    "hookF = [Hook(layer[1]) for layer in list(net._modules.items())]\n",
    "hookB = [Hook(layer[1],backward=True) for layer in list(net._modules.items())]\n",
    "\n",
    "out=net(data)\n",
    "# backprop once to get the backward hook results\n",
    "out.backward(torch.tensor([[1, 1]],dtype=torch.float), retain_graph=True)\n",
    "#! loss.backward(retain_graph=True)  # doesn't work with backward hooks, \n",
    "#! since it's not a network layer but an aggregated result from the outputs of last layer vs target \n",
    "\n",
    "print('***'*3+'  Forward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for i, hook in enumerate(hookF):\n",
    "    print(hook.module)\n",
    "    print('layer {}, input : {}'.format(i+1, hook.input))\n",
    "    print('layer {}, output: {}'.format(i+1, hook.output))\n",
    "    print('---'*17)\n",
    "print('\\n')\n",
    "print('***'*3+'  Backward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for i, hook in enumerate(hookB):\n",
    "    print(hook.module)\n",
    "    print('layer {}, input : {}'.format(i+1, hook.input))\n",
    "    print('layer {}, output: {}'.format(i+1, hook.output))         \n",
    "    print('---'*17)\n",
    "print('\\n')\n",
    "print('***'*3+'      Gradients of parameters      '+'***'*3)\n",
    "for name, p in net.named_parameters():\n",
    "    print(name)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the input and output of forward and backward pass?\n",
    "\n",
    "##### Things to notice:\n",
    "\n",
    "1. Because backward pass runs from back to the start, it's ***parameter order*** should be reversed compared to the forward pass. Therefore, to be it clearer, I'll use a different naming convention below.\n",
    "2. For forward pass, ***previous layer*** of layer 2 is layer1; for backward pass, previous layer of layer 2 is layer 3.\n",
    "3. ***Model output*** is the output of last layer in forward pass.\n",
    "\n",
    "##### `layer.register_backward_hook(module, input, output)`\n",
    "\n",
    "* `input`: previous layer's output\n",
    "* `output`: current layer's output\n",
    "\n",
    "##### `layer.register_backward_hook(module, grad_out, grad_in)`\n",
    "\n",
    "* `grad_in`: gradient of model output wrt. layer output       # from forward pass \n",
    "    * = a tensor that represent the error of each neuron in this layer (= gradient of model output wrt. layer output = how much it should be improved)\n",
    "    * For the last layer: eg. [1, 1] <=> gradient of model output wrt. itself, which means calculate all gradients as normal\n",
    "    * It can also be considered as a weight map: eg. [1, 0] turn off the second gradient; [2, 1] put double weight on first gradient etc.\n",
    "* `grad_out`: `grad_in` * (gradient of layer output wrt. layer input)\n",
    "    * = next layer's error(due to chain rule)\n",
    "    \n",
    "Check the print from the cell above to confirm and enhance your understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_in : tensor([[0.1840, 0.1783]], grad_fn=<ThMulBackward>)\n",
      "grad_out: tensor([[1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# the 4th layer - sigmoid\n",
    "forward_output = hookF[-1].output\n",
    "\n",
    "grad_out = torch.tensor([[1., 1.]])  # sigmoid layer\n",
    "grad_in = grad_out * (forward_output * (1 - forward_output))\n",
    "\n",
    "print('grad_in :', grad_in)\n",
    "print('grad_out:', grad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_in  : tensor([[0.1538, 0.1901]], grad_fn=<MmBackward>)\n",
      "grad_out : tensor([[0.1840, 0.1783]], grad_fn=<ThMulBackward>)\n",
      "grad_w2  : tensor([[0.1094, 0.1097],\n",
      "        [0.1060, 0.1063]], grad_fn=<MmBackward>)\n",
      "grad_b2  : tensor(0.3623, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# the 3th layer - linear\n",
    "grad_out = grad_in\n",
    "grad_in = grad_out @ weight2\n",
    "\n",
    "print('grad_in  :', grad_in)\n",
    "print('grad_out :', grad_out)\n",
    "\n",
    "input = hookF[-2].input[0]\n",
    "\n",
    "# x = [[x_1, x_2]]\n",
    "# grad_out = [[grad_o1, grad_o2]]\n",
    "# grad_(A^T)_ij = grad_o_j * d o_j / d (A^T)_ij = grad_oj * x_i => grad_(A^T) = x^T @ grad_out\n",
    "# grad_w = grad_A = grad_out^T @ x\n",
    "grad_w2 = torch.transpose(grad_out, 0, 1) @ input\n",
    "grad_b2 = torch.sum(grad_out)\n",
    "\n",
    "print('grad_w2  :', grad_w2)\n",
    "print('grad_b2  :', grad_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_in : tensor([[0.0371, 0.0458]], grad_fn=<ThMulBackward>)\n",
      "grad_out: tensor([[0.1538, 0.1901]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# the 2nd layer - sigmoid\n",
    "forward_output = hookF[-3].output\n",
    "\n",
    "grad_out = grad_in\n",
    "grad_in = grad_out * (forward_output * (1 - forward_output))\n",
    "\n",
    "print('grad_in :', grad_in)\n",
    "print('grad_out:', grad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_in  : tensor([[0.0147, 0.0230]], grad_fn=<MmBackward>)\n",
      "grad_out : tensor([[0.0371, 0.0458]], grad_fn=<ThMulBackward>)\n",
      "tensor([[0.0500, 0.1000]])\n",
      "grad_w2  : tensor([[0.0019, 0.0037],\n",
      "        [0.0023, 0.0046]], grad_fn=<MmBackward>)\n",
      "grad_b2  : tensor(0.0828, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# the 1st layer - linear\n",
    "grad_out = grad_in\n",
    "grad_in = grad_out @ weight1\n",
    "\n",
    "print('grad_in  :', grad_in)\n",
    "print('grad_out :', grad_out)\n",
    "\n",
    "input = hookF[0].input[0]\n",
    "\n",
    "grad_w2 = torch.transpose(grad_out, 0, 1) @ input\n",
    "grad_b2 = torch.sum(grad_out)\n",
    "\n",
    "print('grad_w2  :', grad_w2)\n",
    "print('grad_b2  :', grad_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify gradients with hooks\n",
    "\n",
    "Hook function doesn't change gradients by default\n",
    "\n",
    "But if ***return*** is called, the returned value will be the gradient output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU(inplace)\n",
      "  (conv2): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu2): ReLU(inplace)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "net = ConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[0.6557]]]], grad_fn=<ThresholdBackward1>)\n",
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.Tensor([[[[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]]]])\n",
    "data.required_grad = True\n",
    "out = net(data)\n",
    "\n",
    "print(data)\n",
    "print(data.size())\n",
    "print(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********  Forward Hooks Inputs & Outputs  *********\n",
      "Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "layer 1, input : (tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]]),)\n",
      "layer 1, output: tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]], grad_fn=<ThresholdBackward1>)\n",
      "---------------------------------------------------\n",
      "ReLU(inplace)\n",
      "layer 2, input : (tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]], grad_fn=<ThresholdBackward1>),)\n",
      "layer 2, output: tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]], grad_fn=<ThresholdBackward1>)\n",
      "---------------------------------------------------\n",
      "Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "layer 3, input : (tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]], grad_fn=<ThresholdBackward1>),)\n",
      "layer 3, output: tensor([[[[0.6557]]]], grad_fn=<ThresholdBackward1>)\n",
      "---------------------------------------------------\n",
      "ReLU(inplace)\n",
      "layer 4, input : (tensor([[[[0.6557]]]], grad_fn=<ThresholdBackward1>),)\n",
      "layer 4, output: tensor([[[[0.6557]]]], grad_fn=<ThresholdBackward1>)\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "*********  Backward Hooks Inputs & Outputs  *********\n",
      "Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "layer 1, input : (None, tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]]), tensor([0., 0.]))\n",
      "layer 1, output: (tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]]),)\n",
      "---------------------------------------------------\n",
      "ReLU(inplace)\n",
      "layer 2, input : (tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]]),)\n",
      "layer 2, output: (tensor([[[[0.6814]],\n",
      "\n",
      "         [[0.5622]]]]),)\n",
      "---------------------------------------------------\n",
      "Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "layer 3, input : (tensor([[[[0.6814]],\n",
      "\n",
      "         [[0.5622]]]]), tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]]), tensor([1.]))\n",
      "layer 3, output: (tensor([[[[1.]]]]),)\n",
      "---------------------------------------------------\n",
      "ReLU(inplace)\n",
      "layer 4, input : (tensor([[[[1.]]]]),)\n",
      "layer 4, output: (tensor([[[[1.]]]]),)\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "*********      Gradients of parameters      *********\n",
      "conv1.weight\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "conv1.bias\n",
      "tensor([0., 0.])\n",
      "conv2.weight\n",
      "tensor([[[[0.]],\n",
      "\n",
      "         [[0.]]]])\n",
      "conv2.bias\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "hookF = [Hook(layer[1]) for layer in list(net._modules.items())]\n",
    "hookB = [Hook(layer[1],backward=True) for layer in list(net._modules.items())]\n",
    "\n",
    "out = net(data)\n",
    "out.backward(torch.tensor([[[[1]]]], dtype=torch.float))\n",
    "\n",
    "print('***'*3+'  Forward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for i, hook in enumerate(hookF):\n",
    "    print(hook.module)\n",
    "    print('layer {}, input : {}'.format(i+1, hook.input))\n",
    "    print('layer {}, output: {}'.format(i+1, hook.output))\n",
    "    print('---'*17)\n",
    "print('\\n')\n",
    "print('***'*3+'  Backward Hooks Inputs & Outputs  '+'***'*3)\n",
    "for i, hook in enumerate(hookB):\n",
    "    print(hook.module)\n",
    "    print('layer {}, input : {}'.format(i+1, hook.input))\n",
    "    print('layer {}, output: {}'.format(i+1, hook.output))         \n",
    "    print('---'*17)\n",
    "print('\\n')\n",
    "print('***'*3+'      Gradients of parameters      '+'***'*3)\n",
    "for name, p in net.named_parameters():\n",
    "    print(name)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net._modules.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hookB[0].input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_first_layer(model):\n",
    "    first_layer = list(model.children())[0]\n",
    "    \n",
    "    while True:\n",
    "        if isinstance(first_layer, nn.Conv2d):\n",
    "            return first_layer\n",
    "        else:\n",
    "            if not first_layer:\n",
    "                raise ValueError('The first layer is not an `nn.Conv2d` object.')\n",
    "            first_layer = list(first_layer.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GuidedBackpropagation:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.hook_handlers = []\n",
    "        self.relu_forward_outputs = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def first_layer_hook_fn(module, grad_in, grad_out):\n",
    "            self.gradient = grad_in[0]\n",
    "            \n",
    "        def relu_forward_hook_fn(module, ten_in, ten_out):\n",
    "            self.relu_forward_outputs.append(ten_out)\n",
    "            \n",
    "        def relu_backward_hook_fn(module, grad_in, grad_out):\n",
    "            assert len(grad_in) == 1\n",
    "            \n",
    "            features_map = self.relu_forward_outputs[-1]\n",
    "            features_map[features_map > 0] = 1  # in place  # it works\n",
    "            \n",
    "            grad_in = torch.clamp(input=grad_in[0], min=0.0)  # not in place  # it works\n",
    "            grad_in = grad_in * features_map\n",
    "            \n",
    "            del self.relu_forward_outputs[-1]\n",
    "            \n",
    "            return (grad_in,)\n",
    "        \n",
    "        first_layer = get_first_layer(self.model)\n",
    "        handler = first_layer.register_backward_hook(first_layer_hook_fn)\n",
    "        self.hook_handlers.append(handler)\n",
    "        \n",
    "        # The following code can only work in a sequential model\n",
    "        for module in self.model.children():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                handler = module.register_forward_hook(relu_forward_hook_fn)\n",
    "                self.hook_handlers.append(handler)\n",
    "                handler = module.register_backward_hook(relu_backward_hook_fn)\n",
    "                self.hook_handlers.append(handler)\n",
    "            else:\n",
    "                \n",
    "        \n",
    "    def remove(self):\n",
    "        for handler in self.hook_handlers:\n",
    "            handler.remove()\n",
    "        self.hook_handlers = []\n",
    "    \n",
    "    def generate_guided_gradient(self, img, class_idx):\n",
    "        assert isinstance(img, torch.Tensor)\n",
    "        assert img.requires_grad\n",
    "        \n",
    "        output = self.model(img)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        onehot_target = torch.zeros(output.size(), dtype=torch.float)\n",
    "        onehot_target[0][class_idx] = 1\n",
    "        output.backward(gradient=onehot_target)\n",
    "        \n",
    "        guided_gradient = self.gradient.data.numpy()[0]\n",
    "        \n",
    "        self.remove()\n",
    "        \n",
    "        return guided_gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
